#!/usr/bin/env python3
"""
Quick setup script for local development with Ollama
Installs dependencies and provides setup instructions
"""
import os
import sys
import subprocess
import platform

def run_command(cmd, description=""):
    """Run a command and print output"""
    if description:
        print(f"\nğŸ“¦ {description}")
    print(f"   Running: {cmd}")
    result = os.system(cmd)
    if result != 0:
        print(f"   âŒ Command failed!")
        return False
    print(f"   âœ… Done")
    return True

def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   ğŸ¨ Brand Identity Generator - Local Setup with Ollama      â•‘
â•‘   Free, No API Keys Required!                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # Check if Ollama is installed
    print("ğŸ” Checking for Ollama installation...")
    try:
        subprocess.run(["ollama", "--version"], capture_output=True, check=True)
        print("   âœ… Ollama found!")
    except:
        print("""
   âŒ Ollama not found!
   
   Please install Ollama first:
   1. Go to https://ollama.ai
   2. Download and install for your OS
   3. Run this script again
        """)
        sys.exit(1)

    # Check if model is available
    print("\nğŸ¤– Checking for Mistral model...")
    try:
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True, check=True)
        if "mistral" not in result.stdout:
            print("   Model not found. Pulling mistral...")
            os.system("ollama pull mistral")
        else:
            print("   âœ… Mistral model ready!")
    except:
        print("   âš ï¸  Could not verify model. You may need to run: ollama pull mistral")

    # Install backend dependencies
    print("\nğŸ“¦ Setting up backend...")
    backend_path = "backend"
    if os.path.exists(backend_path):
        os.chdir(backend_path)
        if platform.system() == "Windows":
            run_command("pip install -r requirements.txt", "Installing Python dependencies")
        else:
            run_command("pip3 install -r requirements.txt", "Installing Python dependencies")
        os.chdir("..")
    else:
        print(f"   âŒ Backend directory not found at {backend_path}")

    # Install frontend dependencies
    print("\nğŸ“¦ Setting up frontend...")
    frontend_path = "frontend"
    if os.path.exists(frontend_path):
        os.chdir(frontend_path)
        if os.path.exists("package.json"):
            run_command("npm install", "Installing Node.js dependencies")
        else:
            print(f"   âŒ package.json not found in {frontend_path}")
        os.chdir("..")
    else:
        print(f"   âŒ Frontend directory not found at {frontend_path}")

    # Print instructions
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    âœ… Setup Complete!                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ Next Steps:

1. Start Ollama (Terminal 1):
   ollama serve

2. Start Backend (Terminal 2):
   cd backend
   python -m uvicorn main:app --reload

3. Start Frontend (Terminal 3):
   cd frontend
   npm run dev

4. Open browser:
   http://localhost:3000

ğŸ“š For more info, see: LOCAL_SETUP_OLLAMA.md
    """)

if __name__ == "__main__":
    main()
